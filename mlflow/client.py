import pandas as pd
import requests
import joblib
import os

int_fields = [
    "id", "C1", "banner_pos",
    "site_id", "site_domain", "site_category",
    "app_id", "app_domain", "app_category",
    "device_id", "device_ip", "device_model",
    "device_type", "device_conn_type",
    "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21"
]

# ğŸ“„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© (1ì¤„ì”© APIë¡œ ë³´ë‚´ê¸° ìœ„í•¨)
data = pd.read_csv("data.csv", nrows=1, usecols=lambda col: col != "timestamp")
# ğŸ’¡ ëª¨ë¸ ì „ì²˜ë¦¬ìš© ì •ë³´ ì •ì˜ (í•™ìŠµê³¼ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•¨)
target = ['click']
dense_features = ['hour']
sparse_features = [col for col in data.columns if col not in dense_features + target]

# âœ… ì €ì¥ëœ ì „ì²˜ë¦¬ê¸° ë¡œë“œ
ENCODER_DIR = "preprocessors"
encoders = {}
for feat in sparse_features:
    encoders[feat] = joblib.load(os.path.join(ENCODER_DIR, f"{feat}_encoder.pkl"))

scaler = joblib.load(os.path.join(ENCODER_DIR, "minmax_scaler.pkl"))

# âœ… MinMax Scaling (hourì€ floatë¡œ ìœ ì§€)
data[dense_features] = scaler.transform(data[dense_features])
# âœ… ì „ì²˜ë¦¬ ì ìš©
for feat in sparse_features:
    data[feat] = encoders[feat].transform(data[feat].astype(str))

        
# # âœ… ì˜ˆì¸¡ API ì£¼ì†Œ
API_URL = "http://localhost:8000/predict"

# âœ… í•œ ì¤„ì”© APIë¡œ ìš”ì²­
for i, row in data.iterrows():
    input_dict = row[sparse_features + dense_features].to_dict()
    try:
        response = requests.post(API_URL, json=input_dict)
        print(f"[RESPONSE #{i}] Status Code: {response.status_code}")
        result = response.json()
    except Exception as e:
        print(f"[ERROR #{i}] {e}")
        print()

    # (ì˜µì…˜) ëŠë¦° API ì„œë²„ ë°©ì§€ìš© ì§€ì—°
    import time; time.sleep(0.1)
