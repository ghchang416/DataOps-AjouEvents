from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType, FloatType
import joblib
import requests
import redis
import json
from concurrent.futures import ThreadPoolExecutor
import os

KAFKA_BOOTSTRAP_SERVERS = os.environ["KAFKA_BOOTSTRAP_SERVERS"]
KAFKA_TOPIC = os.environ.get("KAFKA_TOPIC", "ajou-ad-clicks-topic")  # defaultê°’ ì§€ì •ë„ ê°€ëŠ¥
KAFKA_SECURITY_PROTOCOL = os.environ["KAFKA_SECURITY_PROTOCOL"]
KAFKA_SASL_MECHANISM = os.environ["KAFKA_SASL_MECHANISM"]
KAFKA_JAAS_CONFIG = os.environ["KAFKA_JAAS_CONFIG"]

REDIS_HOST = os.environ["REDIS_HOST"]
REDIS_PORT = int(os.environ.get("REDIS_PORT", 6379))

# SparkSession ìƒì„±
spark = SparkSession.builder \
    .appName("KafkaPredictionConsumer") \
    .getOrCreate()

# Redis ì—°ê²°
redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)

# âœ… ì „ì²˜ë¦¬ê¸° ë¡œë”©
ENCODER_DIR = "/opt/preprocessors"  # ë³¼ë¥¨ ê²½ë¡œ
sparse_features = [
    "id", "C1", "banner_pos",
    "site_id", "site_domain", "site_category",
    "app_id", "app_domain", "app_category",
    "device_id", "device_ip", "device_model",
    "device_type", "device_conn_type",
    "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21"
] # csvì™€ ë™ì¼í•˜ê²Œ ì‘ì„±
dense_features = ['hour']

encoders = {feat: joblib.load(f"{ENCODER_DIR}/{feat}_encoder.pkl") for feat in sparse_features}
scaler = joblib.load(f"{ENCODER_DIR}/minmax_scaler.pkl")

# âœ… ë©”ì‹œì§€ ìŠ¤í‚¤ë§ˆ ì •ì˜ (Kafka ë©”ì‹œì§€ëŠ” JSON ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¬ ê²ƒ)
schema = StructType()
for feat in sparse_features + dense_features:
    schema = schema.add(feat, StringType() if feat != 'hour' else FloatType())

# âœ… Kafka ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("kafka.security.protocol", KAFKA_SECURITY_PROTOCOL) \
    .option("kafka.sasl.mechanism", KAFKA_SASL_MECHANISM) \
    .option("kafka.sasl.jaas.config", KAFKA_JAAS_CONFIG) \
    .load()

# âœ… Kafka valueëŠ” JSON ë¬¸ìì—´ â†’ structë¡œ ë³€í™˜
json_df = df.selectExpr("CAST(value AS STRING) as json_str") \
    .select(from_json(col("json_str"), schema).alias("data")) \
    .select("data.*")

from concurrent.futures import ThreadPoolExecutor
import threading

# ğŸ”§ ë³‘ë ¬ ìš”ì²­ ë³´ë‚¼ ì“°ë ˆë“œ ìˆ˜
MAX_WORKERS = 8

def send_prediction_request(row):
    try:
        row_dict = row.to_dict()
        raw_id = str(row_dict.pop("raw_id")).split(".")[0]  # âœ… requestì—ì„œëŠ” ì œê±°
        response = requests.post("http://mlflow:8000/predict", json=row_dict, timeout=2)
        result = response.json()
        redis_client.set(f"user:{raw_id}", json.dumps(result))
    except Exception as e:
        print(f"[ERROR] Failed for user_id={row.get('raw_id')}: {e}")

def process_batch(batch_df, epoch_id):
    pdf = batch_df.toPandas()
    if pdf.empty:
        return

    # âœ… ì›ë³¸ ID ë”°ë¡œ ì €ì¥ (requestì— í¬í•¨ X)
    raw_ids = pdf["id"].copy()

    # ğŸ” ì¸ì½”ë”©
    for feat in sparse_features:
        pdf[feat] = encoders[feat].transform(pdf[feat])
    pdf[dense_features] = scaler.transform(pdf[dense_features])

    # âœ… rowì— raw_id í•¨ê»˜ ë„˜ê¸°ê¸°
    pdf["raw_id"] = raw_ids

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        executor.map(send_prediction_request, [row for _, row in pdf.iterrows()])



# âœ… foreachBatchë¡œ ì‹¤í–‰
query = json_df.writeStream \
    .foreachBatch(process_batch) \
    .start()

query.awaitTermination()